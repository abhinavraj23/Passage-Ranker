{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "\n",
    "#Initialize Global variables \n",
    "docIDFDict = {}\n",
    "avgDocLength = 0\n",
    "stemmer = SnowballStemmer('english')\n",
    "totalWordList = []\n",
    "\n",
    "\n",
    "def GetCorpus(inputfile,corpusfile):\n",
    "    f = open(inputfile,\"r\",encoding=\"utf-8\")\n",
    "    fw = open(corpusfile,\"w\",encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        passage = line.strip().lower().split(\"\\t\")[2]\n",
    "        fw.write(passage+\"\\n\")\n",
    "    f.close()\n",
    "    fw.close()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens: stems.append(stemmer.stem(item))\n",
    "    return stems\n",
    "\n",
    "def TfIdfAlgo(corpusfile, delimiter = ' '):\n",
    "    \n",
    "    global totalWordList \n",
    "\n",
    "    for line in open(corpusfile, \"r\", encoding=\"utf-8\"):\n",
    "        totalWordList.append(line)\n",
    "\n",
    "    totalWordList = [\" \".join(tokenize(txt.lower())) for txt in totalWordList]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    matrix = vectorizer.fit_transform(totalWordList).todense()\n",
    "\n",
    "    matrix = pd.DataFrame(matrix, columns = vectorizer.get_feature_names())\n",
    "\n",
    "    top_words = matrix.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    print(top_words)\n",
    "\n",
    "# The following IDF_Generator method reads all the passages(docs) and creates Inverse Document Frequency(IDF) scores for each unique word using below formula \n",
    "# IDF(q_i) = log((N-n(q_i)+0.5)/(n(q_i)+0.5)) where N is the total number of documents in the collection and n(q_i) is the number of documents containing q_i\n",
    "# After finding IDF scores for all the words, The IDF dictionary will be saved in \"docIDFDict.pickle\" file in the current directory\n",
    "\n",
    "def IDF_Generator(corpusfile, delimiter=' ', base=math.e) :\n",
    "\n",
    "    global docIDFDict,avgDocLength\n",
    "\n",
    "    docFrequencyDict = {}       \n",
    "    numOfDocuments = 0   \n",
    "    totalDocLength = 0\n",
    "\n",
    "    for line in open(corpusfile,\"r\",encoding=\"utf-8\") :\n",
    "        doc = line.strip().split(delimiter)\n",
    "        totalDocLength += len(doc)\n",
    "\n",
    "        doc = list(set(doc)) # Take all unique words\n",
    "\n",
    "        for word in doc : #Updates n(q_i) values for all the words(q_i)\n",
    "            if word not in docFrequencyDict :\n",
    "                docFrequencyDict[word] = 0\n",
    "            docFrequencyDict[word] += 1\n",
    "\n",
    "        numOfDocuments = numOfDocuments + 1\n",
    "        if (numOfDocuments%5000==0):\n",
    "            print(numOfDocuments)                \n",
    "\n",
    "    for word in docFrequencyDict:  #Calculate IDF scores for each word(q_i)\n",
    "        docIDFDict[word] = math.log((numOfDocuments - docFrequencyDict[word] +0.5) / (docFrequencyDict[word] + 0.5), base)\n",
    "\n",
    "    avgDocLength = totalDocLength / numOfDocuments\n",
    "\n",
    "     \n",
    "    pickle_out = open(\"docIDFDict.pickle\",\"wb\") # Saves IDF scores in pickle file, which is optional\n",
    "    pickle.dump(docIDFDict, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "\n",
    "    print(\"NumOfDocuments : \", numOfDocuments)\n",
    "    print(\"AvgDocLength : \", avgDocLength)\n",
    "\n",
    "\n",
    "#The following GetBM25Score method will take Query and passage as input and outputs their similarity score based on the term frequency(TF) and IDF values.\n",
    "def GetBM25Score(Query, Passage, k1=1.5, b=0.75, delimiter=' ') :\n",
    "    \n",
    "    global docIDFDict,avgDocLength\n",
    "\n",
    "    query_words= Query.strip().lower().split(delimiter)\n",
    "    passage_words = Passage.strip().lower().split(delimiter)\n",
    "    passageLen = len(passage_words)\n",
    "    docTF = {}\n",
    "    for word in set(query_words):   #Find Term Frequency of all query unique words\n",
    "        docTF[word] = passage_words.count(word)\n",
    "    commonWords = set(query_words) & set(passage_words)\n",
    "    tmp_score = []\n",
    "    for word in commonWords :   \n",
    "        numer = (docTF[word] * (k1+1))   #Numerator part of BM25 Formula\n",
    "        denom = ((docTF[word]) + k1*(1 - b + b*passageLen/avgDocLength)) #Denominator part of BM25 Formula \n",
    "        if(word in docIDFDict) :\n",
    "            tmp_score.append(docIDFDict[word] * numer / denom)\n",
    "\n",
    "    score = sum(tmp_score)\n",
    "    return score\n",
    "\n",
    "#The following line reads each line from testfile and extracts query, passage and calculates BM25 similarity scores and writes the output in outputfile\n",
    "def RunBM25OnEvaluationSet(testfile,outputfile):\n",
    "\n",
    "    lno=0\n",
    "    tempscores=[]  #This will store scores of 10 query,passage pairs as they belong to same query\n",
    "    f = open(testfile,\"r\",encoding=\"utf-8\")\n",
    "    fw = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        tokens = line.strip().lower().split(\"\\t\")\n",
    "        Query = tokens[1]\n",
    "        Passage = tokens[2]\n",
    "        score = GetBM25Score(Query,Passage) \n",
    "        tempscores.append(score)\n",
    "        lno+=1\n",
    "        if(lno%10==0):\n",
    "            tempscores = [str(s) for s in tempscores]\n",
    "            scoreString = \"\\t\".join(tempscores)\n",
    "            qid = tokens[0]\n",
    "            fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "            tempscores=[]\n",
    "        if(lno%5000==0):\n",
    "            print(lno)\n",
    "    print(lno)\n",
    "    f.close()\n",
    "    fw.close()\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "\n",
    "#    inputFileName = \"Data.tsv\"   # This file should be in the following format : queryid \\t query \\t passage \\t label \\t passageid\n",
    "#    testFileName = \"eval1_unlabelled.tsv\"  # This file should be in the following format : queryid \\t query \\t passage \\t passageid # order of the query\n",
    "#    corpusFileName = \"corpus.tsv\"\n",
    "#    outputFileName = \"answer.tsv\"\n",
    "#\n",
    "#    GetCorpus(inputFileName,corpusFileName)    # Gets all the passages(docs) and stores in corpusFile. you can comment this line if corpus file is already generated\n",
    "#    print(\"Corpus File is created.\")\n",
    "#    IDF_Generator(corpusFileName)   # Calculates IDF scores.\n",
    "#    #RunBM25OnTestData(testFileName,outputFileName)\n",
    "#    print(\"IDF Dictionary Generated.\")\n",
    "#    RunBM25OnEvaluationSet(testFileName,outputFileName)\n",
    "#    print(\"Submission file created. \")\n",
    "\n",
    "    TfIdfAlgo(\"corpus.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
